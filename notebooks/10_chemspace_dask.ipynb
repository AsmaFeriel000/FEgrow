{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4c7edd",
   "metadata": {},
   "source": [
    "# FEgrow: An Open-Source Molecular Builder and Free Energy Preparation Workflow\n",
    "\n",
    "**Authors: Mateusz K Bieniek, Ben Cree, Rachael Pirie, Joshua T. Horton, Natalie J. Tatum, Daniel J. Cole**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86bf5e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "For parallelisation we employ Dask. This is done inside of ChemSpace and it spreads the work across all the CPUs/cores on the workstation. \n",
    "\n",
    "It is not just building, parameterising and scoring of the molecules that is parallelised, but also the heaviest parts in the active learning, like the tanimoto distances and fingerprint computation. \n",
    "\n",
    "Dask however can work with more than just one workstation. It can:\n",
    " - schedule jobs on HPC and run them directly there\n",
    " - connect to many different PCs via SSH and run the jobs there\n",
    " - run jobs in the cloud, AWS, and others\n",
    " - and more\n",
    " \n",
    "Here we'll showcase a few options how to tell Dask which computing platform to utilise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a1f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import prody\n",
    "from rdkit import Chem\n",
    "\n",
    "import fegrow\n",
    "from fegrow import ChemSpace\n",
    "\n",
    "from fegrow.testing import core_5R83_path, rec_5R83_path, data_5R83_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53ff25",
   "metadata": {},
   "source": [
    "# Prepare the ligand template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3547f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaffold = Chem.SDMolSupplier(core_5R83_path)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f36f867",
   "metadata": {},
   "source": [
    "As we are using already prepared Smiles that have the scaffold as a substructure, it is not needed to set any growing vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad0d62c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>ALWAYS</b> ensure that <b>__name__ == \"__main__\"</b> when creating a cluster in your code\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b49cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "\n",
    "\n",
    "if False and __name__ == \"__main__\":\n",
    "    lc = LocalCluster(n_workers=2)\n",
    "    # create the chemical space\n",
    "    cs = ChemSpace(dask_cluster=lc)\n",
    "    \n",
    "# from now on you are using your own cluster. And this is very much the default as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8201c7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>SSH workers? </b> Yes, use SSHCluster! </div>\n",
    "You can use SSH to add workstations as workers. First, I recommend setting up your ~/.ssh/config file with your hosts. Then ensure they all the same \"conda environment\", \n",
    "ie same versions of python, dask and other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.dask.org/en/stable/deploying-ssh.html for more\n",
    "from dask.distributed import SSHCluster\n",
    "\n",
    "if False and __name__ == \"__main__\":\n",
    "    lc = SSHCluster(\n",
    "        [\n",
    "        # NOTE: add your public key to ~/.ssh/authorized_keys\n",
    "        'localhost', #  first: scheduler\n",
    "        'localhost', #  workers from now on. Run workers on localhost too\n",
    "        'larch'      #  keep adding worksations, use ~/.ssh/config to define PCs\n",
    "         # NOTE: you can attach many workstations here as a list! \n",
    "        ],\n",
    "        # NOTE: update your firewall (UFW, iptables, firewalld, etc)\n",
    "        #       to allow the scheduler port TCP 8343\n",
    "        scheduler_options={\"port\": 8343, \"dashboard_address\": \":8989\"}, \n",
    "        # processes per host\n",
    "        worker_options={\"n_workers\": 3}, \n",
    "        # best to ensure that the python path is universal across the PCs        \n",
    "        # remote_python='/home/nmb1063/mamba/envs/fegrow/bin/python'\n",
    "       )\n",
    "    \n",
    "    cs = ChemSpace(dask_cluster=lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44157221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://docs.dask.org/en/stable/deploying-ssh.html for more\n",
    "from dask.distributed import SSHCluster\n",
    "\n",
    "if False and __name__ == \"__main__\":\n",
    "    lc = SSHCluster(\n",
    "        [\n",
    "        # NOTE: add your public key to ~/.ssh/authorized_keys\n",
    "        'localhost', #  first: scheduler\n",
    "        'localhost', #  workers from now on. Run workers on localhost too\n",
    "        'larch'      #  keep adding worksations, use ~/.ssh/config to define PCs\n",
    "         # NOTE: you can attach many workstations here as a list! \n",
    "        ],\n",
    "        # NOTE: update your firewall (UFW, iptables, firewalld, etc)\n",
    "        #       to allow the scheduler port TCP 8343\n",
    "        scheduler_options={\"port\": 8343, \"dashboard_address\": \":8989\"}, \n",
    "        # processes per host\n",
    "        worker_options={\"n_workers\": 3}, \n",
    "        # best to ensure that the python path is universal across the PCs        \n",
    "        # remote_python='/home/nmb1063/mamba/envs/fegrow/bin/python'\n",
    "       )\n",
    "    \n",
    "    cs = ChemSpace(dask_cluster=lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example for Archer2 that might be a good start\n",
    "from dask_jobqueue import SLURMCluster  # check out the documentation!\n",
    "import dask\n",
    "\n",
    "def create_archer_cluster():\n",
    "    # Archer has its own instructions for Dask\n",
    "    # and these should be fitted to the jobs run\n",
    "    cluster = SLURMCluster(account='e123-proj', \n",
    "                           queue='standard', \n",
    "                           job_extra_directives=['--nodes=1', '--qos=standard'], \n",
    "                           #n_workers=2,\n",
    "                           #silence_logs='debug',\n",
    "                           processes=8,\n",
    "                           cores=128, \n",
    "                           job_cpu=128, \n",
    "                           memory=\"256GB\", \n",
    "                           job_directives_skip=['--mem', '-n 1', '-N 1'], \n",
    "                           walltime='15:10:00', \n",
    "                           interface='hsn0', \n",
    "                           shebang=\"#!/bin/bash --login\",\n",
    "                           local_directory='$PWD',\n",
    "                           job_script_prologue=['hostname', \n",
    "                                                'ip addr',\n",
    "                                                'eval \"$(/work/../conda shell.bash hook)\"', \n",
    "                                                'conda activate env1', \n",
    "                                                'export OPENMM_CPU_THREADS=1'\n",
    "                                               ], \n",
    "                           scheduler_options={'dashboard_address': 'localhost:9224'})\n",
    "    print(\"JOB Script: \", cluster.job_script(), \"END\")\n",
    "    # request 5 nodes\n",
    "    cluster.scale(jobs=5)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d4e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're not growing the scaffold, we're superimposing bigger molecules on it\n",
    "cs.add_scaffold(scaffold)\n",
    "cs.add_protein(rec_5R83_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b58273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 50k smiles dataset from the study\n",
    "smiles = pd.read_csv(data_5R83_path).Smiles.to_list()\n",
    "\n",
    "# for testing, sort by size and pick small\n",
    "smiles.sort(key=len)\n",
    "# take 5 smallest smiles\n",
    "smiles = smiles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6471a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we add Smiles which should already have been matched\n",
    "# to the scaffold (rdkit Mol.HasSubstructureMatch)\n",
    "cs.add_smiles(smiles[:3], protonate=False)\n",
    "evaluated = cs.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba35d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fegrow",
   "language": "python",
   "name": "fegrow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
